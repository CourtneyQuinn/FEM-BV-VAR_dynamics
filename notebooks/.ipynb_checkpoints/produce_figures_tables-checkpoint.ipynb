{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamical analysis of FEM-BV-VAR model for NAO\n",
    "\n",
    "This notebook contains all the necessary routines for identifying the optimal FEM-BV-VAR model for the NAO and its dynamical properties as presented in the manuscript:\n",
    "\n",
    "\"Dynamical analysis of a reduced model for the NAO\" (Quinn, Harries, and O'Kane, 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import scipy\n",
    "import scipy.linalg as linalg\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from cartopy.util import add_cyclic_point\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.signal import correlate\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "from clustering_dynamics.dynamics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set these as appropriate\n",
    "PROJECT_DIR = os.path.join(os.path.dirname(os.path.abspath('produce_figures_tables.ipynb')),'..')\n",
    "DATA_DIR = os.path.join(PROJECT_DIR,'data')\n",
    "RESULTS_DIR = os.path.join(PROJECT_DIR,'results')\n",
    "FEM_BV_VAR_DIR = os.path.join(RESULTS_DIR,'fembv_varx_fits')\n",
    "EOF_DIR = os.path.join(RESULTS_DIR,'eofs','nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reanalysis = 'nnr1'\n",
    "var_name = 'hgt'\n",
    "var_lev = '500'\n",
    "var_ext = 'anom'\n",
    "timespan = '1948_2018'\n",
    "base_period = [np.datetime64('1979-01-01'), np.datetime64('2018-12-31')]\n",
    "hemisphere = 'NH'\n",
    "region = 'atlantic'\n",
    "season = 'ALL'\n",
    "pc_scaling = 'unit'\n",
    "max_eofs = 200\n",
    "lat_weights = 'scos'\n",
    "\n",
    "base_period_str = '{}_{}'.format(pd.to_datetime(base_period[0]).strftime('%Y%m%d'),\n",
    "                                pd.to_datetime(base_period[1]).strftime('%Y%m%d'))\n",
    "\n",
    "n_PCs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load reanalysis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_filename = '.'.join([var_name, var_lev, timespan, 'nc'])\n",
    "data_file = os.path.join(DATA_DIR, data_filename)\n",
    "\n",
    "hpa500 = xr.open_dataset(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate anomalies based on 1979-2011 climatology\n",
    "base_period_da = hpa500.where(\n",
    "            (hpa500['time'].dt.year >= 1979) &\n",
    "            (hpa500['time'].dt.year <= 2018), drop=True)\n",
    "\n",
    "clim_mean_da = base_period_da.groupby(\n",
    "            base_period_da['time'].dt.dayofyear).mean('time')\n",
    "\n",
    "anom_da = (base_period_da.groupby(\n",
    "            base_period_da['time'].dt.dayofyear) - clim_mean_da)\n",
    "\n",
    "\n",
    "## create data array of anomalies\n",
    "lats = anom_da.variables['lat'][:]\n",
    "lons = anom_da.variables['lon'][:]\n",
    "Zg = anom_da.variables['hgt'][:]\n",
    "\n",
    "roll_to = -lons.argmin()\n",
    "lons = np.roll(lons, roll_to)\n",
    "data = np.roll(Zg.squeeze(), roll_to, axis=-1)\n",
    "\n",
    "data, lons = add_cyclic_point(data, coord=lons)\n",
    "\n",
    "data = xr.DataArray(data[:,0:36,:], coords=[anom_da.time, lats[0:36], lons[:]], \n",
    "                         dims=['time','lat','lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load EOFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eofs_filename = '.'.join([var_name, var_lev, timespan, base_period_str, 'anom', hemisphere, region, base_period_str,\n",
    "                           season, 'max_eofs_{:d}'.format(max_eofs), lat_weights, pc_scaling, 'eofs','nc'])\n",
    "eofs_file = os.path.join(EOF_DIR, eofs_filename)\n",
    "eofs = xr.open_dataset(eofs_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = anom_da.variables['lat'][:]\n",
    "lons = anom_da.variables['lon'][:]\n",
    "Zg = anom_da.variables['hgt'][:]\n",
    "\n",
    "roll_to = -np.argmin(lons.data)\n",
    "lons = np.roll(lons, roll_to)\n",
    "data = np.roll(Zg.squeeze(), roll_to, axis=-1)\n",
    "\n",
    "data, lons = add_cyclic_point(data, coord=lons)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for j in np.arange(0,20):\n",
    "    ax = fig.add_subplot(4, 5, j+1, projection=ccrs.Orthographic(central_longitude=0.0,central_latitude=90.0))\n",
    "    ax.set_global()\n",
    "    lon, lat = np.meshgrid(lons[101:], lats[0:29]) \n",
    "    fill = ax.contourf(lons[101:], lats[0:29],\n",
    "                       eofs.eofs[j,0,0:29,:],\n",
    "                       60, transform=ccrs.PlateCarree(),cmap='PRGn',vmin=-0.1,vmax=0.1)\n",
    "   \n",
    "    ax.set_title('EOF ' + str(j+1))\n",
    "    \n",
    "    # draw coastlines\n",
    "    ax.coastlines()\n",
    "       \n",
    "#plt.tight_layout()\n",
    "\n",
    "#plt.savefig('../figures/figA1.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare FEM-BV-VAR models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## identify optimal model parameters\n",
    "model_prefix = 'hgt.500.1948_2018.{}.anom.{}.{}.{}.ALL.max_eofs_{:d}.scos.unit.fembv_varx.n_pcs{:d}'.format(\n",
    "    base_period_str, hemisphere, region, base_period_str, max_eofs, n_PCs)\n",
    "\n",
    "n_components = [1, 2, 3]\n",
    "memory = [0, 1, 2, 3, 4, 5]\n",
    "state_lengths = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "\n",
    "cv_results = {'n_components': [], 'memory': [], 'state_length': [],\n",
    "              'mean_test_cost': [], 'mean_test_rmse': [], 'mean_test_log_likelihood': [],\n",
    "              'stderr_test_cost': [], 'stderr_test_rmse': [], 'stderr_test_log_likelihood': []}\n",
    "\n",
    "n_samples = None\n",
    "for k in n_components:\n",
    "    for m in memory:\n",
    "        for p in state_lengths:\n",
    "            \n",
    "            model_ds = xr.open_dataset(os.path.join(\n",
    "                FEM_BV_VAR_DIR,\n",
    "                '.'.join([model_prefix, 'k{:d}.m{:d}.state_length{:d}.nc'.format(\n",
    "                k, m, p)])))\n",
    "\n",
    "            if n_samples is None:\n",
    "                n_samples = model_ds.sizes['time']\n",
    "            else:\n",
    "                if model_ds.sizes['time'] != n_samples:\n",
    "                    raise RuntimeError('Number of samples do not match')\n",
    "\n",
    "            cv_results['n_components'].append(k)\n",
    "            cv_results['memory'].append(m)\n",
    "            cv_results['state_length'].append(p)\n",
    "            cv_results['mean_test_cost'].append(model_ds['test_cost'].mean('fold').item())\n",
    "            cv_results['mean_test_rmse'].append(model_ds['test_rmse'].mean('fold').item())\n",
    "            cv_results['mean_test_log_likelihood'].append(model_ds['test_log_likelihood'].mean('fold').item())\n",
    "            cv_results['stderr_test_cost'].append(model_ds['test_cost'].std('fold').item() / np.sqrt(model_ds.sizes['fold']))\n",
    "            cv_results['stderr_test_rmse'].append(model_ds['test_rmse'].std('fold').item() / np.sqrt(model_ds.sizes['fold']))\n",
    "            cv_results['stderr_test_log_likelihood'].append(model_ds['test_log_likelihood'].std('fold').item() / np.sqrt(model_ds.sizes['fold']))\n",
    "\n",
    "            model_ds.close()\n",
    "\n",
    "for f in cv_results:\n",
    "    cv_results[f] = np.asarray(cv_results[f])\n",
    "\n",
    "min_rmse_idx = np.argmin(cv_results['mean_test_rmse'])\n",
    "\n",
    "print('Min. test RMSE k = ', cv_results['n_components'][min_rmse_idx])\n",
    "print('Min. test RMSE m = ', cv_results['memory'][min_rmse_idx])\n",
    "print('Min. test RMSE p = ', cv_results['state_length'][min_rmse_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = plt.gca()\n",
    "\n",
    "axins = inset_axes(ax, width='50%', height='45%', loc=5)\n",
    "\n",
    "unique_n_components = np.unique(n_components)\n",
    "unique_memory = np.unique(memory)\n",
    "\n",
    "n_memory_vals = len(unique_memory)\n",
    "width = 0.8\n",
    "if n_memory_vals % 2 == 0:\n",
    "    offsets = [-j - 0.5 for j in range(0, n_memory_vals // 2)][::-1] + [j + 0.5 for j in range(0, n_memory_vals // 2)]\n",
    "else:\n",
    "    offsets = [-j for j in range(1, n_memory_vals // 2 + 1)][::-1] + [0] + [j for j in range(1, n_memory_vals // 2 + 1)]\n",
    "\n",
    "colors = itertools.cycle(('#fdcc8a', '#fc8d59', '#d7301f', '#fef0d9'))\n",
    "linestyles = itertools.cycle(('-', '--', ':', '-.'))\n",
    "\n",
    "for k in unique_n_components:\n",
    "    \n",
    "    c = next(colors)\n",
    "    ls = next(linestyles)\n",
    "\n",
    "    markers = itertools.cycle(('.', 'x', 's', 'd', 'v', '^', '<', '>'))\n",
    "    \n",
    "    for i, m in enumerate(unique_memory):\n",
    "        \n",
    "        marker = next(markers)\n",
    "        \n",
    "        mask = np.logical_and(cv_results['n_components'] == k, cv_results['memory'] == m)\n",
    "        \n",
    "        xcoords = cv_results['state_length'][mask] + offsets[i] * width\n",
    "        cv_mean = cv_results['mean_test_rmse'][mask]\n",
    "        cv_std_err = cv_results['stderr_test_rmse'][mask]\n",
    "        \n",
    "        ax.errorbar(xcoords, cv_mean, yerr=cv_std_err, capsize=5, markersize=8, color=c, ls='none', marker=marker,\n",
    "                    label='$K = {:d}, m = {:d}$'.format(k, m))\n",
    "        \n",
    "        axins.errorbar(xcoords, cv_mean, yerr=cv_std_err, capsize=5, markersize=8, color=c, ls='none', marker=marker)\n",
    "\n",
    "ax.legend(ncol=3, fontsize=14, bbox_to_anchor=(0.5, -0.3), loc='center', borderaxespad=0.)\n",
    "\n",
    "ax.grid(ls='--', color='gray', alpha=0.5)\n",
    "axins.grid(ls='--', color='gray', alpha=0.5)\n",
    "\n",
    "ax.tick_params(axis='both', labelsize=14)\n",
    "ax.set_xlabel('$p$ (days)', fontsize=16)\n",
    "ax.set_ylabel('Test set RMSE', fontsize=16)\n",
    "\n",
    "axins.set_ylim(180, 200)\n",
    "axins.set_xlim(-3, 25)\n",
    "axins.tick_params(axis='both', labelsize=14)\n",
    "\n",
    "ax.set_title('$d = 20$, $N_{init} = 20$, $N_{folds} = 10$', fontsize=18)\n",
    "       \n",
    "#plt.savefig('../figures/fig1.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of optimal FEM-BV-VAR model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "m = 3\n",
    "p = 5\n",
    "\n",
    "model_filename = '.'.join([var_name, var_lev, timespan, base_period_str, 'anom', hemisphere, region, base_period_str,\n",
    "                           season, 'max_eofs_{:d}'.format(max_eofs), lat_weights, pc_scaling, 'fembv_varx',\n",
    "                           'n_pcs20','k{:d}'.format(k),'m{:d}'.format(m),'state_length{:d}'.format(p),'nc'])\n",
    "model_file = os.path.join(FEM_BV_VAR_DIR, model_filename)\n",
    "model = xr.open_dataset(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting state composites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_state_assignments(weights_da, time_name='time', state_name='fembv_state'):\n",
    "\n",
    "    n_samples = weights_da.sizes[time_name]\n",
    "\n",
    "    state_axis = weights_da.get_axis_num(state_name)\n",
    "\n",
    "    if state_axis != 1:\n",
    "        weights_da = weights_da.transpose(time_name, state_name)\n",
    "\n",
    "    weights = weights_da\n",
    "    mask = np.all(np.isfinite(weights), axis=1)\n",
    "    \n",
    "    valid_weights = weights[mask]\n",
    "    \n",
    "    valid_viterbi = np.argmax(valid_weights.data, axis=1)\n",
    "    \n",
    "    full_viterbi = np.full((n_samples,), np.NaN)\n",
    "    full_viterbi[mask] = valid_viterbi\n",
    "\n",
    "    viterbi = xr.DataArray(\n",
    "        full_viterbi,\n",
    "        coords={time_name: weights_da[time_name]},\n",
    "        dims=[time_name], name=state_name)\n",
    "    \n",
    "    return viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fembv_state_composites(model_ds, anom_da, bootstrap=True, bootstrap_type='independent',\n",
    "                                     n_bootstrap=1000, time_name='time', random_seed=None):\n",
    "    \"\"\"Calculate FEM-BV-VARX state composites.\"\"\"\n",
    "    \n",
    "    random_state = np.random.default_rng(random_seed)\n",
    "    n_components = model_ds.sizes['fembv_state']\n",
    "\n",
    "    affs = model['weights'].dropna(time_name)\n",
    "    affs_start = affs[time_name].min()\n",
    "    affs_end = affs[time_name].max()\n",
    "    \n",
    "    viterbi = viterbi_state_assignments(affs)\n",
    "    \n",
    "    anom_da = anom_da.where(\n",
    "        (anom_da[time_name] >= affs_start) & (anom_da[time_name] <= affs_end),\n",
    "        drop=True)\n",
    "        \n",
    "    composites_da = anom_da.groupby(viterbi).mean(time_name)\n",
    "    \n",
    "    if not bootstrap:\n",
    "        return composites_da\n",
    "        \n",
    "    n_samples = viterbi.sizes[time_name]\n",
    "\n",
    "    percentile_scores_da = xr.zeros_like(composites_da)\n",
    "    \n",
    "    if bootstrap_type == 'independent':\n",
    "        for k in range(n_components):\n",
    "            \n",
    "            n_events = np.sum(viterbi == k).item()\n",
    "            \n",
    "            bootstrap_composites = []\n",
    "            for s in range(n_bootstrap):\n",
    "                t_boot = random_state.choice(n_samples, size=n_events, replace=False)\n",
    "                bootstrap_composites.append(anom_da.isel({time_name: t_boot}).mean(time_name).squeeze())\n",
    "    \n",
    "            bootstrap_composites = xr.concat(bootstrap_composites, dim='bootstrap_sample')\n",
    "\n",
    "            composite_dims = list(composites_da.sel(fembv_state=k).squeeze().dims)\n",
    "            composite_coords = composites_da.sel(fembv_state=k).squeeze().coords\n",
    "        \n",
    "            # ensure sample dimension is first dimension\n",
    "            bootstrap_composites = bootstrap_composites.transpose(*(['bootstrap_sample'] + composite_dims))\n",
    "        \n",
    "            original_shape = [composites_da.sizes[d] for d in composite_dims]\n",
    "            n_features = np.prod(original_shape)\n",
    "        \n",
    "            flat_composite = np.reshape(composites_da.sel(fembv_state=k).data, (n_features,))\n",
    "            flat_bootstrap_composites = np.reshape(bootstrap_composites.data, (n_bootstrap, n_features))\n",
    "        \n",
    "            scores = np.zeros((n_features,), dtype=np.float64)\n",
    "            for i in range(n_features):\n",
    "                scores[i] = stats.percentileofscore(flat_bootstrap_composites[:, i], flat_composite[i], kind='weak') / 100.0\n",
    "   \n",
    "            scores_da = xr.DataArray(np.reshape(scores, original_shape), coords=composite_coords, dims=composite_dims)\n",
    "        \n",
    "            percentile_scores_da.loc[dict(fembv_state=k)] = scores_da\n",
    "    \n",
    "    elif bootstrap_type == 'multinomial':\n",
    "\n",
    "        bootstrap_composites = {k: [] for k in range(n_components)}\n",
    "        \n",
    "        for s in range(n_bootstrap):\n",
    "            \n",
    "            t = list(np.arange(n_samples))\n",
    "            \n",
    "            for k in range(n_components):\n",
    "                n_events = np.sum(viterbi == k).item()\n",
    "                t_boot = random_state.choice(t, size=n_events, replace=False)\n",
    "                t = [ti for ti in t if ti not in t_boot]\n",
    "                bootstrap_composites[k].append(anom_da.isel({time_name: t_boot}).mean(time_name).squeeze())\n",
    "\n",
    "            assert len(t) == 0\n",
    "            \n",
    "        for k in range(n_components):\n",
    "\n",
    "            bootstrap_composites[k] = xr.concat(bootstrap_composites[k], dim='bootstrap_sample')\n",
    "\n",
    "            composite_dims = list(composites_da.sel(fembv_state=k).squeeze().dims)\n",
    "            composite_coords = composites_da.sel(fembv_state=k).squeeze().coords\n",
    "        \n",
    "            # ensure sample dimension is first dimension\n",
    "            bootstrap_composites[k] = bootstrap_composites[k].transpose(*(['bootstrap_sample'] + composite_dims))\n",
    "        \n",
    "            original_shape = [composites_da.sizes[d] for d in composite_dims]\n",
    "            n_features = np.prod(original_shape)\n",
    "        \n",
    "            flat_composite = np.reshape(composites_da.sel(fembv_state=k).data, (n_features,))\n",
    "            flat_bootstrap_composites = np.reshape(bootstrap_composites[k].data, (n_bootstrap, n_features))\n",
    "        \n",
    "            scores = np.zeros((n_features,), dtype=np.float64)\n",
    "            for i in range(n_features):\n",
    "                scores[i] = stats.percentileofscore(flat_bootstrap_composites[:, i], flat_composite[i], kind='weak') / 100.0\n",
    "   \n",
    "            scores_da = xr.DataArray(np.reshape(scores, original_shape), coords=composite_coords, dims=composite_dims)\n",
    "        \n",
    "            percentile_scores_da.loc[dict(fembv_state=k)] = scores_da\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized bootstrap method '%r'\" % bootstrap_type)\n",
    "\n",
    "    composites_ds = xr.Dataset({'composites': composites_da, 'bootstrap_percentile': percentile_scores_da})\n",
    "\n",
    "    return composites_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "bootstrap = True\n",
    "fembv_composites = calculate_fembv_state_composites(model, anom_da['hgt'], bootstrap=bootstrap, bootstrap_type='multinomial', n_bootstrap=10,\n",
    "                                                    random_seed=0)\n",
    "alpha = 0.01\n",
    "if bootstrap and isinstance(fembv_composites, xr.Dataset):     \n",
    "    fembv_composites = xr.where((fembv_composites['bootstrap_percentile'] >= 1.0 - 0.5 * alpha) |\n",
    "                                (fembv_composites['bootstrap_percentile'] <= 0.5 * alpha), fembv_composites['composites'], np.NaN)\n",
    "    \n",
    "end_time = time.time()\n",
    "elapsed = (end_time-start_time)/60\n",
    "print(\"Elapsed time: {} min\".format(round(elapsed,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_composites = fembv_composites.sizes['fembv_state']\n",
    "n_cols = n_composites\n",
    "n_rows = 1\n",
    "wrap_lon = True\n",
    "    \n",
    "projection = ccrs.Orthographic(central_latitude=90, central_longitude=0)\n",
    "\n",
    "vmins = np.full((n_composites,), fembv_composites.min().item())\n",
    "vmaxs = np.full((n_composites,), fembv_composites.max().item())\n",
    "\n",
    "height_ratios = np.ones((n_rows + 1))\n",
    "height_ratios[-1] = 0.1\n",
    "\n",
    "fig = plt.figure(constrained_layout=False, figsize=(4 * n_cols, 4 * n_rows))\n",
    "\n",
    "gs = gridspec.GridSpec(ncols=n_cols, nrows=n_rows + 1, figure=fig,\n",
    "                       wspace=0.05, hspace=0.2,\n",
    "                       height_ratios=height_ratios)\n",
    "\n",
    "lat = fembv_composites['lat']\n",
    "lon = fembv_composites['lon']\n",
    "\n",
    "row_index = 0\n",
    "col_index = 0\n",
    "\n",
    "\n",
    "for i in range(n_composites):\n",
    "\n",
    "    composite_data = fembv_composites.sel(fembv_state=i).squeeze().values\n",
    "\n",
    "    vmin = np.nanmin(composite_data)\n",
    "    vmax = np.nanmax(composite_data)\n",
    "    \n",
    "    ax_vmin = -max(np.abs(vmin), np.abs(vmax))\n",
    "    ax_vmax = -ax_vmin\n",
    "\n",
    "    if wrap_lon:\n",
    "        composite_data, composite_lon = add_cyclic_point(composite_data, coord=lon)\n",
    "    else:\n",
    "        composite_lon = lon\n",
    "\n",
    "    lon_grid, lat_grid = np.meshgrid(composite_lon, lat)\n",
    "\n",
    "    ax = fig.add_subplot(gs[row_index, col_index], projection=projection)\n",
    "\n",
    "    ax.coastlines()\n",
    "    ax.set_global()\n",
    "\n",
    "    cs = ax.pcolor(lon_grid, lat_grid, composite_data, vmin=ax_vmin, vmax=ax_vmax,\n",
    "                   cmap=plt.cm.RdBu_r, transform=ccrs.PlateCarree())\n",
    "\n",
    "    if np.any(~np.isfinite(composite_data)):\n",
    "        ax.patch.set_facecolor('lightgray')\n",
    "\n",
    "    cb_ax = fig.add_subplot(gs[-1, col_index])\n",
    "    cb = fig.colorbar(cs, cax=cb_ax, pad=0.05, orientation='horizontal')\n",
    "    cb.set_label(r'$Z_{g500\\,\\mathrm{hPa}}^\\prime$ (gpm)', fontsize=14)\n",
    "\n",
    "    ax.set_title('state {}'.format(i+1), fontsize=14)\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    col_index += 1\n",
    "    if col_index == n_cols:\n",
    "        col_index = 0\n",
    "        row_index += 1\n",
    "\n",
    "#plt.savefig('../figures/fig2.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating by state and identifying transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## separate affiliation indices by state\n",
    "comp1_ind = np.where(model.weights[5:].argmax(dim = 'fembv_state') == 0)[0]+5\n",
    "comp2_ind = np.where(model.weights[5:].argmax(dim = 'fembv_state') == 1)[0]+5\n",
    "comp3_ind = np.where(model.weights[5:].argmax(dim = 'fembv_state') == 2)[0]+5\n",
    "\n",
    "## extract transition indices (last day in state)\n",
    "affil_seq = model.weights[5:].argmax(dim = 'fembv_state')\n",
    "trans_ind_all = np.array([],dtype=int)\n",
    "state_length_all = np.array([],dtype=int)\n",
    "\n",
    "for i in np.arange(0,affil_seq.shape[0]-1):\n",
    "    if affil_seq[i] != affil_seq[i+1]:\n",
    "        trans_ind_all = np.append(trans_ind_all,i+5)\n",
    "\n",
    "## extract residency times\n",
    "state_length_all = np.empty(trans_ind_all.shape[0]+1,dtype=int)\n",
    "        \n",
    "for i in np.arange(0,trans_ind_all.shape[0]+1):\n",
    "    if i == 0:\n",
    "        state_length_all[i] = trans_ind_all[i]-5+1\n",
    "    elif i == trans_ind_all.shape[0]:\n",
    "        state_length_all[i] = model.weights.shape[0]-trans_ind_all[i-1]-1\n",
    "    else:\n",
    "        state_length_all[i] = trans_ind_all[i]-trans_ind_all[i-1]\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## separate transitions and residencies by state\n",
    "trans_ind_1 = np.array([],dtype=int)\n",
    "trans_ind_2 = np.array([],dtype=int)\n",
    "trans_ind_3 = np.array([],dtype=int)\n",
    "\n",
    "state_length_1 = np.array([],dtype=int)\n",
    "state_length_2 = np.array([],dtype=int)\n",
    "state_length_3 = np.array([],dtype=int)\n",
    "\n",
    "for i in np.arange(0,trans_ind_all.shape[0]):\n",
    "    state_affil_i = affil_seq[trans_ind_all[i]-5]\n",
    "    if state_affil_i == 0:\n",
    "        trans_ind_1 = np.append(trans_ind_1,trans_ind_all[i])\n",
    "        state_length_1 = np.append(state_length_1,state_length_all[i])\n",
    "    elif state_affil_i == 1:\n",
    "        trans_ind_2 = np.append(trans_ind_2,trans_ind_all[i])\n",
    "        state_length_2 = np.append(state_length_2,state_length_all[i])\n",
    "    elif state_affil_i == 2:\n",
    "        trans_ind_3 = np.append(trans_ind_3,trans_ind_all[i])\n",
    "        state_length_3 = np.append(state_length_3,state_length_all[i])\n",
    "    else:\n",
    "        print('invalid state at index {}'.format(i))\n",
    "        break\n",
    "\n",
    "## classify final state residence\n",
    "if np.isin(model.weights.shape[0]-1,comp1_ind):\n",
    "    state_length_1 = np.append(state_length_1,state_length_all[-1])\n",
    "elif np.isin(model.weights.shape[0]-1,comp2_ind):\n",
    "    state_length_2 = np.append(state_length_2,state_length_all[-1])\n",
    "elif np.isin(model.weights.shape[0]-1,comp3_ind):\n",
    "    state_length_3 = np.append(state_length_3,state_length_all[-1])\n",
    "else:\n",
    "    print('invalid state at index {}'.format(i))\n",
    "\n",
    "## separate transitions by state transitioned to\n",
    "trans_ind_to_1 = np.array([],dtype=int)\n",
    "trans_ind_to_2 = np.array([],dtype=int)\n",
    "trans_ind_to_3 = np.array([],dtype=int)\n",
    "\n",
    "for i in np.arange(0,trans_ind_all.shape[0]):\n",
    "    state_affil_i = affil_seq[trans_ind_all[i]+1-5]\n",
    "    if state_affil_i == 0:\n",
    "        trans_ind_to_1 = np.append(trans_ind_to_1,trans_ind_all[i]+1)\n",
    "    elif state_affil_i == 1:\n",
    "        trans_ind_to_2 = np.append(trans_ind_to_2,trans_ind_all[i]+1)\n",
    "    elif state_affil_i == 2:\n",
    "        trans_ind_to_3 = np.append(trans_ind_to_3,trans_ind_all[i]+1)\n",
    "    else:\n",
    "        print('invalid state at index {}'.format(i))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assign times in states and at transitions\n",
    "state_1_times = model.time[comp1_ind]\n",
    "state_2_times = model.time[comp2_ind]\n",
    "state_3_times = model.time[comp3_ind]\n",
    "\n",
    "trans_1_times = model.time[trans_ind_1]\n",
    "trans_2_times = model.time[trans_ind_2]\n",
    "trans_3_times = model.time[trans_ind_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate statistics by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert residencies to xarray and add time coordinates\n",
    "trans_inds_collect = [trans_ind_1,trans_ind_2,trans_ind_3]\n",
    "state_length_inds = [[]]*k\n",
    "for ii in np.arange(0,k):\n",
    "    if affil_seq[-1] == ii:\n",
    "        state_length_inds[ii] = np.append(trans_inds_collect[ii],-1)\n",
    "    else:\n",
    "        state_length_inds[ii] = trans_inds_collect[ii]\n",
    "\n",
    "state_length_1   = xr.DataArray(state_length_1, coords=[model.time[state_length_inds[0]]], dims=['time'])\n",
    "state_length_2   = xr.DataArray(state_length_2, coords=[model.time[state_length_inds[1]]], dims=['time'])\n",
    "state_length_3   = xr.DataArray(state_length_3, coords=[model.time[state_length_inds[2]]], dims=['time'])\n",
    "\n",
    "state_length_collect = [state_length_1,state_length_2,state_length_3]\n",
    "\n",
    "means = np.zeros((len(state_length_collect),5))\n",
    "mins = np.zeros((len(state_length_collect),5))\n",
    "maxs = np.zeros((len(state_length_collect),5))\n",
    "\n",
    "seasons = ['DJF','MAM','JJA','SON','ALL']\n",
    "\n",
    "for si in np.arange(0,len(seasons)):\n",
    "    if seasons[si] == 'ALL':\n",
    "        for jj in np.arange(0,len(state_length_collect)):\n",
    "            means[jj,si] = np.mean(state_length_collect[jj])\n",
    "            mins[jj,si]  = np.min(state_length_collect[jj])\n",
    "            maxs[jj,si]  = np.max(state_length_collect[jj])\n",
    "    else:\n",
    "        for jj in np.arange(0,len(state_length_collect)): \n",
    "            seasonal_state_lengths = state_length_collect[jj].where(model.time.dt.season==seasons[si],drop=True)\n",
    "            means[jj,si] = np.mean(seasonal_state_lengths)\n",
    "            mins[jj,si]  = np.min(seasonal_state_lengths)\n",
    "            maxs[jj,si]  = np.max(seasonal_state_lengths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.row_stack([mins[0,:],means[0,:],maxs[0,:],\n",
    "                                  mins[1,:],means[1,:],maxs[1,:],\n",
    "                                  mins[2,:],means[2,:],maxs[2,:]]), \n",
    "                  index=['state 1 min','state 1 mean','state 1 max','state 2 min','state 2 mean','state 2 max',\n",
    "                         'state 3 min','state 3 mean','state 3 max'],\n",
    "                  columns=['DJF','MAM','JJA','SON','All'])\n",
    "\n",
    "h_styles = [dict(selector=\"th\", props=[(\"font-size\", \"12pt\")])]\n",
    "df.style.set_table_styles(h_styles).set_properties(**{'font-size': '14pt'}).format(\"{:,.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal behaviour of states and transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## separate by specific transition\n",
    "trans_ind_1_to_2 = np.array([],dtype=int)\n",
    "trans_ind_1_to_3 = np.array([],dtype=int)\n",
    "trans_ind_2_to_1 = np.array([],dtype=int)\n",
    "trans_ind_2_to_3 = np.array([],dtype=int)\n",
    "trans_ind_3_to_1 = np.array([],dtype=int)\n",
    "trans_ind_3_to_2 = np.array([],dtype=int)\n",
    "\n",
    "for ti in trans_ind_1:\n",
    "    if np.isin(ti+1,trans_ind_to_2):\n",
    "        trans_ind_1_to_2 = np.append(trans_ind_1_to_2,ti)\n",
    "    elif np.isin(ti+1,trans_ind_to_3):\n",
    "        trans_ind_1_to_3 = np.append(trans_ind_1_to_3,ti)\n",
    "\n",
    "for ti in trans_ind_2:\n",
    "    if np.isin(ti+1,trans_ind_to_1):\n",
    "        trans_ind_2_to_1 = np.append(trans_ind_2_to_1,ti)\n",
    "    elif np.isin(ti+1,trans_ind_to_3):\n",
    "        trans_ind_2_to_3 = np.append(trans_ind_2_to_3,ti)\n",
    "\n",
    "for ti in trans_ind_3:\n",
    "    if np.isin(ti+1,trans_ind_to_1):\n",
    "        trans_ind_3_to_1 = np.append(trans_ind_3_to_1,ti)\n",
    "    elif np.isin(ti+1,trans_ind_to_2):\n",
    "        trans_ind_3_to_2 = np.append(trans_ind_3_to_2,ti)\n",
    "\n",
    "trans_1_to_2_times = model.time[trans_ind_1_to_2]\n",
    "trans_1_to_3_times = model.time[trans_ind_1_to_3]\n",
    "trans_2_to_1_times = model.time[trans_ind_2_to_1]\n",
    "trans_2_to_3_times = model.time[trans_ind_2_to_3]\n",
    "trans_3_to_1_times = model.time[trans_ind_3_to_1]\n",
    "trans_3_to_2_times = model.time[trans_ind_3_to_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of transitions\n",
    "trans_num_season = np.zeros((7,5),dtype=np.float)\n",
    "\n",
    "trans_ind_collect = [trans_ind_1_to_2,trans_ind_1_to_3,\n",
    "                     trans_ind_2_to_1,trans_ind_2_to_3,\n",
    "                     trans_ind_3_to_1,trans_ind_3_to_2]\n",
    "\n",
    "seasons = ['DJF','MAM','JJA','SON']\n",
    "\n",
    "for si in np.arange(0,len(seasons)):\n",
    "    for jj in np.arange(0,len(trans_ind_collect)):\n",
    "        trans_times_season = model.time[trans_ind_collect[jj]].where(model.time.dt.season==seasons[si],drop=True)\n",
    "        trans_num_season[jj,si] = trans_times_season.shape[0]\n",
    "\n",
    "trans_num_season[6,:] = np.sum(trans_num_season[0:6,:],axis=0)\n",
    "trans_num_season[:,4] = np.sum(trans_num_season[:,0:4],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## days in each state\n",
    "state_times_season = np.zeros((4,5),dtype=np.float)\n",
    "\n",
    "state_times_collect = [state_1_times,state_2_times,state_3_times,model.time[5:]]\n",
    " \n",
    "for si in np.arange(0,len(seasons)):\n",
    "    for jj in np.arange(0,len(state_times_collect)):   \n",
    "        state_times_season[jj,si] = state_times_collect[jj].where(model.time.dt.season==seasons[si],drop=True).shape[0]\n",
    "\n",
    "state_times_season[:,4] = np.sum(state_times_season[:,0:4],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.concatenate([trans_num_season,state_times_season],axis=0), \n",
    "                  columns=np.append(seasons,'All seasons'),\n",
    "                  index=['1 to 2','1 to 3','2 to 1','2 to 3','3 to 1','3 to 2','Any trans',\n",
    "                         'state 1','state 2','state 3','Any state'])\n",
    "\n",
    "h_styles = [dict(selector=\"th\", props=[(\"font-size\", \"12pt\")])]\n",
    "df.style.set_table_styles(h_styles).set_properties(**{'font-size': '14pt'}).format(\"{:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cpc = 'NAO'\n",
    "IND_cpc = np.genfromtxt(os.path.join(DATA_DIR,'cpc.{}.daily.csv'.format(index_cpc.lower())), delimiter=',')\n",
    "\n",
    "## calculating time variable\n",
    "IND_cpc_time = np.zeros(IND_cpc.shape[0],dtype='datetime64[s]')\n",
    "for tt in np.arange(0,IND_cpc.shape[0]):\n",
    "    IND_cpc_time[tt] = np.datetime64('{}-{}-{}'.format(int(IND_cpc[tt,0]),\n",
    "                       str(int(IND_cpc[tt,1])).zfill(2),str(int(IND_cpc[tt,2])).zfill(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding window residecy percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Teleconnection index\n",
    "\n",
    "## sliding window anomaly residency\n",
    "y = 0\n",
    "end_ind = IND_cpc.shape[0]-365\n",
    "IND_cpc_pos = np.empty(end_ind)\n",
    "IND_cpc_neg = np.empty(end_ind)\n",
    "\n",
    "for y_ind in np.arange(0,end_ind):\n",
    "    IND_cpc_pos[y] = np.count_nonzero(IND_cpc[y_ind:y_ind+365,3]>=0)/365\n",
    "    IND_cpc_neg[y] = np.count_nonzero(IND_cpc[y_ind:y_ind+365,3]<=0)/365\n",
    "    \n",
    "    y += 1\n",
    "\n",
    "## Convert to xarray\n",
    "IND_cpc_pos = xr.DataArray(IND_cpc_pos, coords=[IND_cpc_time[365:]], dims=['time'])\n",
    "IND_cpc_neg = xr.DataArray(IND_cpc_neg, coords=[IND_cpc_time[365:]], dims=['time'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model fit index\n",
    "\n",
    "## sliding window anomaly residency\n",
    "n_components = k\n",
    "days = 365\n",
    "    \n",
    "y = 0\n",
    "end_ind = model.time.shape[0]-days-5\n",
    "comp_freq_sw = np.empty((end_ind,k))\n",
    "\n",
    "for y_ind in np.arange(0,end_ind):\n",
    "    for state in np.arange(0,k):\n",
    "        comp_freq_sw[y,state] = np.count_nonzero(model.weights[5+y_ind:5+y_ind+days].argmax(dim='fembv_state') == state,\n",
    "                                                 axis=0)/days\n",
    "    y += 1\n",
    "    \n",
    "## convert to xarray\n",
    "comp_freq_sw = xr.DataArray(comp_freq_sw, coords=[model.time[5+365:],np.arange(1,4)], dims=['time','state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yearly average and LOWESS fit residency percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Teleconnection index\n",
    "\n",
    "## percentage of calendar year spent in negative NAO state\n",
    "start_year = 0\n",
    "y = 0\n",
    "IND_binned = np.empty((IND_cpc_time.shape[0]))\n",
    "num_years = round(IND_cpc_time.shape[0]/365)\n",
    "year_inds_IND = np.zeros(num_years, dtype=int)\n",
    "\n",
    "for year in np.arange(2021-num_years,2021):\n",
    "    if year == 2020:\n",
    "        days = 182\n",
    "    elif np.mod(year,4)==0:\n",
    "        days = 366\n",
    "    else:\n",
    "        days = 365\n",
    "    \n",
    "    IND_binned[start_year:start_year+days] = np.ones(days)*np.count_nonzero(IND_cpc[start_year:start_year+days,3]<0)/days\n",
    "    year_inds_IND[y] = int(start_year)\n",
    "    \n",
    "    start_year += days\n",
    "    y += 1\n",
    "\n",
    "\n",
    "## convert to xarray\n",
    "IND_binned = xr.DataArray(IND_binned, coords=[IND_cpc_time], dims=['time'])\n",
    "\n",
    "## LOWESS fit\n",
    "IND_binned_lowess = np.empty((year_inds_IND.shape[0]))\n",
    "binned_lowess_p = lowess(IND_binned[year_inds_IND].sel(time=slice(\"1979-01-01\", \"2018-12-31\")),\n",
    "                         IND_binned[year_inds_IND].time.sel(time=slice(\"1979-01-01\", \"2018-12-31\")),frac=0.25)\n",
    "IND_binned_lowess = binned_lowess_p[:,1]\n",
    "\n",
    "IND_binned_lowess = xr.DataArray(IND_binned_lowess, \n",
    "                                 coords=[IND_binned[year_inds_IND+181].time.sel(time=slice(\"1979-01-01\", \"2018-12-31\"))], dims=['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model fit index\n",
    "## choose index from in\n",
    "model_NAO_ind = 1\n",
    "\n",
    "## percentage of calendar year spent in negative NAO state\n",
    "affil_binned = np.empty((model.time.shape[0]-5,k))\n",
    "num_years = round(model.time.shape[0]/365)\n",
    "year_inds = np.zeros(num_years, dtype=int)\n",
    "affil_binned_lowess = np.zeros((year_inds.shape[0],k))\n",
    "\n",
    "start_year = 0\n",
    "y = 0\n",
    "for year in np.arange(2019-num_years,2019):\n",
    "    if np.mod(year,4)==0:\n",
    "        days = 366\n",
    "    elif year == 1979:\n",
    "        days = 360\n",
    "    else:\n",
    "        days = 365\n",
    "    \n",
    "    for state in np.arange(0,k):\n",
    "        affil_binned[start_year:start_year+days,state] = np.ones(days)*(np.count_nonzero(model.weights[5+start_year:5+start_year+days].argmax(dim='fembv_state') == state,\n",
    "                                                                               axis=0)/days)\n",
    "    if y == 0:\n",
    "        year_inds[y] = int(start_year)\n",
    "    else:\n",
    "        year_inds[y] = int(start_year)+5\n",
    "    \n",
    "    start_year += days\n",
    "    y += 1\n",
    "    \n",
    "## LOWESS fit\n",
    "for state in np.arange(0,k):\n",
    "    binned_lowess_p = lowess(affil_binned[year_inds,state],model.time[year_inds],frac=0.25)\n",
    "    affil_binned_lowess[:,state] = binned_lowess_p[:,1]\n",
    "\n",
    "## convert to xarray\n",
    "affil_binned = xr.DataArray(affil_binned, coords=[model.time[5:], np.arange(0,k)], dims=['time','fembv_state'])\n",
    "affil_binned_lowess = xr.DataArray(affil_binned_lowess, coords=[model.time[year_inds+181], np.arange(0,k)], dims=['time','fembv_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## choose state index to compare\n",
    "state_ind = 1\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "\n",
    "ax1 = fig.add_subplot(2,1,1)\n",
    "ax1.plot(IND_cpc_neg.time, IND_cpc_neg)\n",
    "ax1.plot(comp_freq_sw.time, comp_freq_sw[:,state_ind])\n",
    "\n",
    "ax1.set_xlim([np.datetime64(\"1980-01-01\"),np.datetime64(\"2018-12-31\")])\n",
    "ax1.set_ylim(0.0, 1.05)\n",
    "ax1.tick_params(axis='both', labelsize=13)\n",
    "ax1.grid(ls='--', color='gray', alpha=0.5)\n",
    "\n",
    "ax1.set_xlabel('Year', fontsize=14)\n",
    "ax1.set_ylabel('Residency fraction', fontsize=14)\n",
    "plt.title('Residency percent (365 day sliding window)',fontsize=15)\n",
    "plt.legend(['CPC NAO$^-$ index','state {}'.format(state_ind+1)],loc='upper right')\n",
    "plt.tight_layout()\n",
    "\n",
    "ax2 = fig.add_subplot(2,1,2)\n",
    "p1 = ax2.plot(IND_binned.time, IND_binned,alpha=0.6)\n",
    "color1 = p1[0].get_color()\n",
    "ax2.plot(IND_binned_lowess.time, IND_binned_lowess,'--',color=color1,lw=3)\n",
    "p2 = ax2.plot(affil_binned.time, affil_binned[:,state_ind],alpha=0.6)\n",
    "color2 = p2[0].get_color()\n",
    "ax2.plot(affil_binned_lowess.time, affil_binned_lowess[:,state_ind],'--',color=color2,lw=3)\n",
    "\n",
    "ax2.set_xlim([np.datetime64(\"1980-01-01\"),np.datetime64(\"2018-12-31\")])\n",
    "ax2.set_ylim(0.0, 1.05)\n",
    "ax2.grid(ls='--', color='gray', alpha=0.5)\n",
    "ax2.tick_params(axis='both', labelsize=13)\n",
    "\n",
    "ax2.set_xlabel('Year', fontsize=14)\n",
    "ax2.set_ylabel('Residency fraction', fontsize=14)\n",
    "plt.title('Residency percent (yearly average and LOWESS)',fontsize=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('../figures/fig3.pdf'.format(region,k,m,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation(y1, y2):\n",
    "    \"\"\"Calculate correlation of one variable with another.\"\"\"\n",
    "    start_time = max(y1.index.min(), y2.index.min())\n",
    "    end_time = min(y1.index.max(), y2.index.max())\n",
    "    y1_da = y1.where((y1.index >= start_time) & (y1.index <= end_time)).dropna()\n",
    "    y2_da = y2.where((y2.index >= start_time) & (y2.index <= end_time)).dropna()\n",
    "    mask = np.logical_or(np.isnan(y1_da.to_numpy()), np.isnan(y2_da.to_numpy()))\n",
    "    y1_values = np.ma.masked_array(y1_da.to_numpy(), mask=mask)\n",
    "    y2_values = np.ma.masked_array(y2_da.to_numpy(), mask=mask)\n",
    "    correlation = stats.pearsonr(y1_values, y2_values)[0]\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation to NAO (negative) index\n",
    "corr_cpc = calculate_correlation(IND_cpc_neg.to_pandas(),comp_freq_sw[:,1].to_pandas())\n",
    "\n",
    "# correlation to binned NAO (negative) index\n",
    "corr_cpc_binned = calculate_correlation(IND_binned.to_pandas(),affil_binned[:,1].to_pandas()) \n",
    "\n",
    "# correlation to low-pass filter NAO (negative) index\n",
    "corr_cpc_binned_lowess = calculate_correlation(IND_binned_lowess.to_pandas(),affil_binned_lowess[:,1].to_pandas())    \n",
    "\n",
    "df = pd.DataFrame(np.round([corr_cpc,corr_cpc_binned,corr_cpc_binned_lowess],2), \n",
    "                  index=['no filter','binned','binned and LOWESS'],columns=[str(p)+' days'])\n",
    "\n",
    "df.style.set_caption('Correlations with NAO neg index').set_properties(**{'font-size': '12pt'}).format(\"{:.2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute matrix cocycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_len = model.weights.shape[0]-5\n",
    "state_space = m*n_PCs\n",
    "A = np.array(model.A)\n",
    "gammas = np.array(model.weights)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "AT = np.matmul(gammas[:,:],A[:,0,:,:].transpose(1, 0, 2)).transpose(0,2,1)\n",
    "for mm in np.arange(1,m):\n",
    "    AT = np.concatenate((AT,np.matmul(gammas[:,:],A[:,mm,:,:].transpose(1, 0, 2)).transpose(0,2,1)),axis=1)\n",
    "\n",
    "I0 = np.concatenate((np.eye(n_PCs*(m-1)),np.zeros((n_PCs*(m-1),n_PCs))),axis=1)\n",
    "I0 = np.repeat(I0[:, :, np.newaxis], AT.shape[2], axis=2)\n",
    "\n",
    "matrix_cocycle = np.concatenate((AT,I0),axis=0)\n",
    "matrix_cocycle = matrix_cocycle[:,:,5:]\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end-start\n",
    "print(\"Elapsed time: {} sec\".format(round(elapsed,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CLVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ms = [3,10,30,50]\n",
    "state_space = m*n_PCs\n",
    "CLVs_all = [[]]*len(Ms)\n",
    "i = 0\n",
    "\n",
    "for M in Ms:\n",
    "    \n",
    "    CLVs_filename = '.'.join([var_name, var_lev, timespan, base_period_str, 'anom', hemisphere, region, 'ALL', \n",
    "                         'max_eofs_{:d}'.format(max_eofs), lat_weights, pc_scaling, 'm{:d}'.format(m),\n",
    "                          'state_length{:d}'.format(p),'CLVs', 'M{:d}'.format(M),'orth1','nc'])\n",
    "\n",
    "    CLVs_file = os.path.join(FEM_BV_VAR_DIR, 'CLVs','truncated', CLVs_filename)\n",
    "\n",
    "    CLVs_ds = xr.open_dataset(CLVs_file)\n",
    "    CLVs = CLVs_ds['CLVs']\n",
    "    \n",
    "    CLVs_all[i] = CLVs\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load time for CLV calculations\n",
    "time_CLVs_all = [[]]*len(Ms)\n",
    "\n",
    "for i in np.arange(0,len(Ms)):\n",
    "    CLVs = CLVs_all[i]\n",
    "    time_CLVs_all[i] = CLVs.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate FTCLEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lyaps_all = [[]]*len(Ms)\n",
    "\n",
    "for Mi in np.arange(0,len(Ms)):\n",
    "    M = Ms[Mi]\n",
    "    CLVs = CLVs_all[Mi]\n",
    "    time_CLVs = time_CLVs_all[Mi]\n",
    "\n",
    "    M_FTLE  = 1\n",
    "    orth_win = 1\n",
    "    Nk = np.arange(0,M_FTLE+1,orth_win)\n",
    "    Qp = np.eye(state_space)\n",
    "    Lyaps = np.empty((CLVs.shape[1],CLVs.shape[2]))\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for i in np.arange(0,CLVs.shape[2]):\n",
    "        C = np.array(CLVs[:,:,i])\n",
    "        norm_C = linalg.norm(C,axis=0)\n",
    "        Lyap_i = np.empty((CLVs.shape[1],M_FTLE))\n",
    "        for tt in np.arange(0,M_FTLE):\n",
    "            C = np.matmul(matrix_cocycle[:,:,i+tt+M],C)\n",
    "            Lyap_i[:,tt] = linalg.norm(C,axis=0)-norm_C\n",
    "            norm_C = linalg.norm(C,axis=0)\n",
    "        Lyaps[:,i] = np.mean(Lyap_i,axis=1)\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = end-start\n",
    "    print(\"Elapsed time: {} sec, M = {}\".format(round(elapsed,4),M))\n",
    "    \n",
    "    Lyaps_all[Mi] = Lyaps  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to xarray\n",
    "for Mi in np.arange(0,len(Ms)):\n",
    "    Lyaps_all[Mi] = xr.DataArray(Lyaps_all[Mi], coords=[np.arange(1,CLVs_all[Mi].shape[1]+1), time_CLVs_all[Mi]], dims=['FTLE', 'time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate asymptotic Lyapunov exponents using QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lyap_asymp = calculate_FTLEs(state_space,matrix_cocycle,np.arange(0,matrix_cocycle.shape[2]+1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13,10))\n",
    "\n",
    "for j in np.arange(0,len(Ms)):\n",
    "    mean_Lyaps = np.mean(Lyaps_all[j][:,5:],axis=1)\n",
    "    min_Lyaps = np.min(Lyaps_all[j][:,5:],axis=1)\n",
    "    max_Lyaps = np.max(Lyaps_all[j][:,5:],axis=1)\n",
    "    std_Lyaps = np.std(Lyaps_all[j][:,5:],axis=1)\n",
    "    \n",
    "    ax = fig.add_subplot(len(Ms), 1, j+1)\n",
    "\n",
    "    for ll in range(0,10):\n",
    "        pl = ax.errorbar(ll+1, mean_Lyaps[ll], yerr=std_Lyaps[ll], fmt='o',lw=3,capsize=8,markeredgewidth=2);\n",
    "        c = pl[0].get_color()\n",
    "        ax.plot(ll+1.1, Lyap_asymp[ll],'o',ms=8,color=c,fillstyle='none',mew=2);\n",
    "        ax.plot(ll+1, min_Lyaps[ll],'D',ms=8,color=c);\n",
    "        ax.plot(ll+1, max_Lyaps[ll],'s',ms=8,color=c);\n",
    "    mean = ax.errorbar(np.nan,np.nan,yerr=np.nan,fmt='o',lw=3,capsize=8,markeredgewidth=2,color=[0,0,0],label='mean/std');\n",
    "    handles = [mean,\n",
    "            matplotlib.lines.Line2D([],[],marker='D',ms=8,color=[0,0,0],linestyle='none'),\n",
    "           matplotlib.lines.Line2D([],[],marker='s',ms=8,color=[0,0,0],linestyle='none'),\n",
    "           matplotlib.lines.Line2D([],[],marker='o',ms=8,color=[0,0,0],fillstyle='none',mew=2,linestyle='none')]\n",
    "    if j == 0:\n",
    "        ax.legend(handles,('mean/std','min','max','asymp'),ncol=4)\n",
    "    ax.set_ylabel('$\\Lambda_i$')\n",
    "    ax.set_title('\\n M = {}'.format(Ms[j]),fontsize = 13)\n",
    "    ax.grid()\n",
    "    if j == (len(Ms)-1):\n",
    "        ax.set_xlabel('i')\n",
    "    else:\n",
    "        ax.get_xaxis().set_ticklabels([])\n",
    "    ax.set_ylim([-0.75, 0.25])  \n",
    "\n",
    "    plt.tight_layout\n",
    "    \n",
    "#plt.savefig('../figures/fig4.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DimKY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimKY_all = [[]]*len(Ms)\n",
    "\n",
    "for Mi in np.arange(0,len(Ms)):\n",
    "    M = Ms[Mi]\n",
    "    Lyaps = Lyaps_all[Mi]\n",
    "    CLVs = CLVs_all[Mi]\n",
    "    time_CLVs = time_CLVs_all[Mi]\n",
    "    \n",
    "    dimKY = np.empty((CLVs.shape[2]))\n",
    "\n",
    "    t = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    for tt in time_CLVs:\n",
    "        Lyaps_t = Lyaps.sel(time = tt)[:10]\n",
    "        Lyaps_ord = Lyaps_t.sortby(Lyaps_t,ascending=False)\n",
    "        for ll in np.arange(1,Lyaps_ord.shape[0]):\n",
    "            S = np.sum(Lyaps_ord[:ll])\n",
    "            if S<0:\n",
    "                i_min = ll-1\n",
    "                break\n",
    "            elif ll == CLVs.shape[1]:\n",
    "                i_min = ll-1\n",
    "    \n",
    "        dimKY[t] = i_min + np.sum(Lyaps_ord[:i_min])/abs(Lyaps_ord[i_min])\n",
    "        t += 1\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = end-start\n",
    "    print(\"Elapsed time: {} sec, M = {}\".format(round(elapsed,4),M))\n",
    "    \n",
    "    dimKY_all[Mi] = dimKY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to xarray\n",
    "for Mi in np.arange(0,len(Ms)):\n",
    "    dimKY_all[Mi] = xr.DataArray(dimKY_all[Mi], coords=[time_CLVs_all[Mi]], dims=['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilities of positive dimension by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimKY_pos_all = np.array(np.zeros(len(Ms)))\n",
    "dimKY_pos_state1_all = np.array(np.zeros(len(Ms)))\n",
    "dimKY_pos_state2_all = np.array(np.zeros(len(Ms)))\n",
    "dimKY_pos_state3_all = np.array(np.zeros(len(Ms)))  \n",
    "\n",
    "M_labs = [[]]*len(Ms)\n",
    "\n",
    "for M_ii in np.arange(0,len(Ms)):\n",
    "    M_labs[M_ii] = 'M = {}'.format(Ms[M_ii])\n",
    "    \n",
    "    ## extract dimKY for push forward step\n",
    "    dimKY = dimKY_all[M_ii]\n",
    "    \n",
    "    ## probability dimKY is positive\n",
    "    dimKY_pos = (np.sum(dimKY>0))/dimKY.shape[0]\n",
    "    \n",
    "    ## extract times in each state where dynamics are calculated\n",
    "    state1_times_CLVs = state_1_times.where(state_1_times == time_CLVs_all[M_ii],drop=True)\n",
    "    state2_times_CLVs = state_2_times.where(state_2_times == time_CLVs_all[M_ii],drop=True)\n",
    "    state3_times_CLVs = state_3_times.where(state_3_times == time_CLVs_all[M_ii],drop=True)\n",
    "\n",
    "    ## given each state, probability dimKY positive \n",
    "    dimKY_pos_state1 = np.sum(dimKY.sel(time = state1_times_CLVs)>0)/state1_times_CLVs.shape[0]\n",
    "    dimKY_pos_state2 = np.sum(dimKY.sel(time = state2_times_CLVs)>0)/state2_times_CLVs.shape[0]\n",
    "    dimKY_pos_state3 = np.sum(dimKY.sel(time = state3_times_CLVs)>0)/state3_times_CLVs.shape[0]\n",
    "\n",
    "    ## store values for table\n",
    "    dimKY_pos_all[M_ii] = dimKY_pos\n",
    "    dimKY_pos_state1_all[M_ii] = dimKY_pos_state1\n",
    "    dimKY_pos_state2_all[M_ii] = dimKY_pos_state2\n",
    "    dimKY_pos_state3_all[M_ii] = dimKY_pos_state3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.round([dimKY_pos_state1_all,dimKY_pos_state2_all,dimKY_pos_state3_all,\n",
    "                            dimKY_pos_all],4), \n",
    "                  index=['P(FTLE > 0 | state 1)',\n",
    "                           'P(FTLE > 0 | state 2)','P(FTLE > 0 | state 3)','P(FTLE > 0)'],\n",
    "                  columns=M_labs)\n",
    "\n",
    "h_styles = [dict(selector=\"th\", props=[(\"font-size\", \"12pt\")])]\n",
    "df.style.set_table_styles(h_styles).set_properties(**{'font-size': '14pt'}).format(\"{:,.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average dimension by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_times_collection = [state_1_times, state_2_times, state_3_times]\n",
    "comp_ind_collection = [comp1_ind, comp2_ind, comp3_ind]\n",
    "\n",
    "dimKY_state_avg = np.zeros((3,2))\n",
    "state_inds_long = [[]]*3\n",
    "\n",
    "for jj in np.arange(0,len(state_times_collection)):\n",
    "    dimKY_state_avg[jj,0] = np.mean(dimKY_all[0].where(dimKY_all[0].time == state_times_collection[jj],drop=True))\n",
    "    \n",
    "    state_inds_long_temp = np.array([],dtype=int)\n",
    "    \n",
    "    for ii in comp_ind_collection[jj]:\n",
    "        if np.all(np.isin(model.time[ii-2:ii+3],state_times_collection[jj])) == True:\n",
    "            state_inds_long_temp = np.append(state_inds_long_temp,ii)\n",
    "    \n",
    "    state_inds_long[jj] = state_inds_long_temp\n",
    "    dimKY_state_avg[jj,1] = np.mean(dimKY_all[0].where(dimKY_all[0].time == model.time[state_inds_long[jj]],drop=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.round(dimKY_state_avg,4), \n",
    "                  index=['state 1','state 2','state 3'],\n",
    "                  columns=['no filter','5 day filter'])\n",
    "\n",
    "h_styles = [dict(selector=\"th\", props=[(\"font-size\", \"12pt\")])]\n",
    "df.style.set_table_styles(h_styles).set_properties(**{'font-size': '14pt'}).format(\"{:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_CLVs_all = [[]]*len(Ms)\n",
    "align_all = [[]]*len(Ms)\n",
    "\n",
    "num_CLV_test = 6\n",
    "\n",
    "for i in np.arange(0,len(Ms)):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    M = Ms[i]\n",
    "    CLVs = CLVs_all[i]\n",
    "    \n",
    "    num_CLVs = CLVs.shape[2]\n",
    "    time_CLVs = time_CLVs_all[i]\n",
    "    align = np.array(np.zeros((num_CLV_test,num_CLV_test,num_CLVs),dtype=np.float))\n",
    "\n",
    "    for t in np.arange(0,num_CLVs):\n",
    "        for clvi in np.arange(0,num_CLV_test):\n",
    "            for clvj in np.arange(clvi+1,num_CLV_test+1):\n",
    "                align[clvi,clvj-1,t] = abs(np.dot(np.array(CLVs[:,clvi,t]),np.array(CLVs[:,clvj,t])))\n",
    "    \n",
    "    num_CLVs_all[i] = num_CLVs\n",
    "    align_all[i] = align\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = (end-start)/60\n",
    "    print(\"Elapsed time: {} min, M = {}\".format(round(elapsed,4),M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to xarray\n",
    "for i in np.arange(0,len(Ms)):\n",
    "    M = Ms[i]\n",
    "    CLVs = CLVs_all[i]\n",
    "    time_CLVs = time_CLVs_all[i]\n",
    "    align = align_all[i]\n",
    "    \n",
    "    align_all[i] = xr.DataArray(align, coords=[np.arange(1,num_CLV_test+1),np.arange(2,num_CLV_test+2), time_CLVs], dims=['CLV_i','CLV_j', 'time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "\n",
    "fig = plt.figure(figsize=(10,16))\n",
    "(ax1, ax2, ax3, ax4, ax5, ax6) = fig.subplots(6, 1, gridspec_kw={'height_ratios': [2, 1, 1, 2, 1, 1]})\n",
    "\n",
    "axes_collect = np.array([[ax1, ax2, ax3],[ax4, ax5, ax6]]).T\n",
    "plot_titles = ['(a)','(b)']\n",
    "\n",
    "for pi in np.arange(0,axes_collect.shape[1]):\n",
    "    axes = axes_collect[:,pi]\n",
    "    \n",
    "    axes[0].plot(model.time[comp1_ind], model.weights[comp1_ind,0]*0.6,'ko')\n",
    "    axes[0].plot(model.time[comp2_ind], model.weights[comp2_ind,1]*0.55,'ks')\n",
    "    axes[0].plot(model.time[comp3_ind], model.weights[comp3_ind,2]*0.5,'kd')\n",
    "    axes[0].plot(align_all[j].time,align_all[j][0,0,:].T)\n",
    "    axes[0].plot(align_all[j].time,align_all[j][1,1,:].T)\n",
    "    axes[0].plot(align_all[j].time,align_all[j][0,1,:].T)\n",
    "    axes[0].legend(['state 1','state 2','state 3','$\\\\theta_{1,2}$','$\\\\theta_{2,3}$','$\\\\theta_{1,3}$'])\n",
    "\n",
    "    for kk in np.arange(0,3):\n",
    "        axes[1].plot(time_CLVs_all[j],Lyaps_all[j][kk,:],'C{}'.format(kk))\n",
    "    axes[1].legend(['$\\Lambda_1$','$\\Lambda_2$','$\\Lambda_3$'])\n",
    "\n",
    "    axes[2].plot(time_CLVs_all[j],dimKY_all[j],'.-')\n",
    "    axes[2].set_xlabel(plot_titles[pi],fontsize = 20)\n",
    "    if pi == 0:\n",
    "        axes[2].legend(['dim_KY'],loc = 'lower right')\n",
    "    else:\n",
    "        axes[2].legend(['dim_KY'])\n",
    "    \n",
    "    \n",
    "    for axii in np.arange(0,axes_collect.shape[0]):\n",
    "        if pi == 0:\n",
    "            axes[axii].set_xlim([np.datetime64(\"2012-03-15\"),np.datetime64(\"2012-08-01\")])\n",
    "        else: \n",
    "            axes[axii].set_xlim([np.datetime64(\"1993-11-01\"),np.datetime64(\"1994-03-15\")])\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "#plt.savefig('../figures/fig7.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Extracting transitions associated with persistent states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract transitions associated with long states\n",
    "char_time = 4\n",
    "\n",
    "trans_ind_1_long = np.array([],dtype=int)\n",
    "trans_ind_2_long = np.array([],dtype=int)\n",
    "trans_ind_3_long = np.array([],dtype=int)\n",
    "\n",
    "trans_ind_to_1_long = np.array([],dtype=int)\n",
    "trans_ind_to_2_long = np.array([],dtype=int)\n",
    "trans_ind_to_3_long = np.array([],dtype=int)\n",
    "\n",
    "for ii in np.arange(0,trans_ind_all.shape[0]):\n",
    "    if state_length_all[ii] > char_time:    \n",
    "        if np.isin(trans_ind_all[ii],trans_ind_1):\n",
    "            trans_ind_1_long = np.append(trans_ind_1_long,trans_ind_all[ii])\n",
    "        elif np.isin(trans_ind_all[ii],trans_ind_2):\n",
    "            trans_ind_2_long = np.append(trans_ind_2_long,trans_ind_all[ii])\n",
    "        elif np.isin(trans_ind_all[ii],trans_ind_3):\n",
    "            trans_ind_3_long = np.append(trans_ind_3_long,trans_ind_all[ii])\n",
    "        else:\n",
    "            print('error: invalid transition index')\n",
    "    if state_length_all[ii+1] > char_time:\n",
    "        if np.isin(trans_ind_all[ii]+1,trans_ind_to_1):\n",
    "            trans_ind_to_1_long = np.append(trans_ind_to_1_long,trans_ind_all[ii]+1)\n",
    "        elif np.isin(trans_ind_all[ii]+1,trans_ind_to_2):\n",
    "            trans_ind_to_2_long = np.append(trans_ind_to_2_long,trans_ind_all[ii]+1)\n",
    "        elif np.isin(trans_ind_all[ii]+1,trans_ind_to_3):\n",
    "            trans_ind_to_3_long = np.append(trans_ind_to_3_long,trans_ind_all[ii]+1)\n",
    "        else:\n",
    "            print('error: invalid transition index')\n",
    "        \n",
    "trans_1_long_times = model.time[trans_ind_1_long]\n",
    "trans_2_long_times = model.time[trans_ind_2_long]\n",
    "trans_3_long_times = model.time[trans_ind_3_long]\n",
    "\n",
    "trans_to_1_long_times = model.time[trans_ind_to_1_long]\n",
    "trans_to_2_long_times = model.time[trans_ind_to_2_long]\n",
    "trans_to_3_long_times = model.time[trans_ind_to_3_long]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## categorize by specific transition\n",
    "trans_ind_1_to_2_long = np.array([],dtype=int)\n",
    "trans_ind_1_to_3_long = np.array([],dtype=int)\n",
    "trans_ind_2_to_1_long = np.array([],dtype=int)\n",
    "trans_ind_2_to_3_long = np.array([],dtype=int)\n",
    "trans_ind_3_to_1_long = np.array([],dtype=int)\n",
    "trans_ind_3_to_2_long = np.array([],dtype=int)\n",
    "\n",
    "for ti in trans_ind_1_long:\n",
    "    if np.isin(ti+1,trans_ind_to_2_long):\n",
    "        trans_ind_1_to_2_long = np.append(trans_ind_1_to_2_long,ti)\n",
    "    elif np.isin(ti+1,trans_ind_to_3_long):\n",
    "        trans_ind_1_to_3_long = np.append(trans_ind_1_to_3_long,ti)\n",
    "\n",
    "for ti in trans_ind_2_long:\n",
    "    if np.isin(ti+1,trans_ind_to_1_long):\n",
    "        trans_ind_2_to_1_long = np.append(trans_ind_2_to_1_long,ti)\n",
    "    elif np.isin(ti+1,trans_ind_to_3_long):\n",
    "        trans_ind_2_to_3_long = np.append(trans_ind_2_to_3_long,ti)\n",
    "\n",
    "for ti in trans_ind_3_long:\n",
    "    if np.isin(ti+1,trans_ind_to_1_long):\n",
    "        trans_ind_3_to_1_long = np.append(trans_ind_3_to_1_long,ti)\n",
    "    elif np.isin(ti+1,trans_ind_to_2_long):\n",
    "        trans_ind_3_to_2_long = np.append(trans_ind_3_to_2_long,ti)\n",
    "\n",
    "trans_1_to_2_long_times = model.time[trans_ind_1_to_2_long]\n",
    "trans_1_to_3_long_times = model.time[trans_ind_1_to_3_long]\n",
    "trans_2_to_1_long_times = model.time[trans_ind_2_to_1_long]\n",
    "trans_2_to_3_long_times = model.time[trans_ind_2_to_3_long]\n",
    "trans_3_to_1_long_times = model.time[trans_ind_3_to_1_long]\n",
    "trans_3_to_2_long_times = model.time[trans_ind_3_to_2_long]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting alignment behaviour associated with transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select push forward step (here M=3)\n",
    "Mi = 0\n",
    "M = Ms[Mi]\n",
    "align = align_all[Mi]\n",
    "CLVs = CLVs_all[Mi]\n",
    "time_CLVs = time_CLVs_all[Mi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract alignment for days around transitions\n",
    "start_ind = 5\n",
    "end_ind = -5\n",
    "\n",
    "trans_1_to_2_inds_CLVs = trans_ind_1_to_2_long[np.where(trans_1_to_2_long_times.isin(time_CLVs[start_ind:end_ind]))[0]]\n",
    "align_trans_1_to_2 = np.array(np.zeros((num_CLV_test,num_CLV_test,trans_1_to_2_inds_CLVs.shape[0],10),dtype=np.float))\n",
    "\n",
    "trans_1_to_3_inds_CLVs = trans_ind_1_to_3_long[np.where(trans_1_to_3_long_times.isin(time_CLVs[start_ind:end_ind]))[0]]\n",
    "align_trans_1_to_3 = np.array(np.zeros((num_CLV_test,num_CLV_test,trans_1_to_3_inds_CLVs.shape[0],10),dtype=np.float))\n",
    "\n",
    "trans_2_to_1_inds_CLVs = trans_ind_2_to_1_long[np.where(trans_2_to_1_long_times.isin(time_CLVs[start_ind:end_ind]))[0]]\n",
    "align_trans_2_to_1 = np.array(np.zeros((num_CLV_test,num_CLV_test,trans_2_to_1_inds_CLVs.shape[0],10),dtype=np.float))\n",
    "\n",
    "trans_2_to_3_inds_CLVs = trans_ind_2_to_3_long[np.where(trans_2_to_3_long_times.isin(time_CLVs[start_ind:end_ind]))[0]]\n",
    "align_trans_2_to_3 = np.array(np.zeros((num_CLV_test,num_CLV_test,trans_2_to_3_inds_CLVs.shape[0],10),dtype=np.float))\n",
    "\n",
    "trans_3_to_1_inds_CLVs = trans_ind_3_to_1_long[np.where(trans_3_to_1_long_times.isin(time_CLVs[start_ind:end_ind]))[0]]\n",
    "align_trans_3_to_1 = np.array(np.zeros((num_CLV_test,num_CLV_test,trans_3_to_1_inds_CLVs.shape[0],10),dtype=np.float))\n",
    "\n",
    "trans_3_to_2_inds_CLVs = trans_ind_3_to_2_long[np.where(trans_3_to_2_long_times.isin(time_CLVs[start_ind:end_ind]))[0]]\n",
    "align_trans_3_to_2 = np.array(np.zeros((num_CLV_test,num_CLV_test,trans_3_to_2_inds_CLVs.shape[0],10),dtype=np.float))\n",
    "\n",
    "\n",
    "for dd in np.arange(end_ind,start_ind):\n",
    "    align_trans_1_to_2[:,:,:,dd-end_ind] = align.sel(time = model.time[trans_1_to_2_inds_CLVs-dd])\n",
    "    align_trans_1_to_3[:,:,:,dd-end_ind] = align.sel(time = model.time[trans_1_to_3_inds_CLVs-dd])\n",
    "    align_trans_2_to_1[:,:,:,dd-end_ind] = align.sel(time = model.time[trans_2_to_1_inds_CLVs-dd])\n",
    "    align_trans_2_to_3[:,:,:,dd-end_ind] = align.sel(time = model.time[trans_2_to_3_inds_CLVs-dd])\n",
    "    align_trans_3_to_1[:,:,:,dd-end_ind] = align.sel(time = model.time[trans_3_to_1_inds_CLVs-dd])\n",
    "    align_trans_3_to_2[:,:,:,dd-end_ind] = align.sel(time = model.time[trans_3_to_2_inds_CLVs-dd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = plt.gca()\n",
    "\n",
    "align_trans_all = np.concatenate([align_trans_1_to_2, align_trans_1_to_3, \n",
    "                                  align_trans_2_to_1, align_trans_2_to_3,\n",
    "                                  align_trans_3_to_1, align_trans_3_to_2],axis=2)\n",
    "\n",
    "data = np.concatenate([align_trans_all[0,0,:,:],align_trans_all[1,1,:,:],align_trans_all[0,1,:,:]],axis=0)\n",
    "\n",
    "data = np.reshape(data,(data.shape[0]*data.shape[1],1))\n",
    "\n",
    "pair_labs = np.expand_dims(np.repeat(np.concatenate([np.repeat('$\\\\theta_{1,2}$',align_trans_all.shape[2]),\n",
    "            np.repeat('$\\\\theta_{2,3}$',align_trans_all.shape[2]),\n",
    "            np.repeat('$\\\\theta_{1,3}$',align_trans_all.shape[2])],axis=0),align_trans_all.shape[3],axis=0),axis=1)\n",
    "\n",
    "day_labs = np.expand_dims(np.tile(np.flip(np.arange(end_ind+1,start_ind+1)),align_trans_all.shape[2]*3),axis=1)\n",
    "\n",
    "## create the pandas DataFrame \n",
    "df = pd.DataFrame(np.concatenate([data,pair_labs,day_labs],axis=1),columns = ['alignment','pair','day']) \n",
    "df['alignment'] = pd.to_numeric(df['alignment'])\n",
    "df['day'] = pd.to_numeric(df['day'])\n",
    "    \n",
    "sns.boxplot(x = 'day', y = 'alignment', hue = 'pair', data=df)\n",
    "\n",
    "ax.set_xticklabels(np.arange(end_ind+1,start_ind+1))\n",
    "ax.set_title('all transitions',fontsize=12)\n",
    "ax.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('../figures/fig8.eps') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,12))\n",
    "\n",
    "align_collections = [align_trans_1_to_2, align_trans_1_to_3, \n",
    "                     align_trans_2_to_1, align_trans_2_to_3,\n",
    "                     align_trans_3_to_1, align_trans_3_to_2]\n",
    "\n",
    "titles  = ['from 1 to 2 ({} samples)'.format(trans_1_to_2_inds_CLVs.shape[0]),\n",
    "           'from 1 to 3 ({} samples)'.format(trans_1_to_3_inds_CLVs.shape[0]),\n",
    "           'from 2 to 1 ({} samples)'.format(trans_2_to_1_inds_CLVs.shape[0]),\n",
    "           'from 2 to 3 ({} samples)'.format(trans_2_to_3_inds_CLVs.shape[0]),\n",
    "           'from 3 to 1 ({} samples)'.format(trans_3_to_1_inds_CLVs.shape[0]),\n",
    "           'from 3 to 2 ({} samples)'.format(trans_3_to_2_inds_CLVs.shape[0])]\n",
    "\n",
    "for j in np.arange(0,6):\n",
    "    ax = fig.add_subplot(6,1,j+1)\n",
    "    align_to_plot = align_collections[j]\n",
    "    \n",
    "    p1 = ax.plot(np.flip(np.arange(end_ind+1,start_ind+1)),align_to_plot[0,0,:,:].T,color='C0')\n",
    "    p2 = ax.plot(np.flip(np.arange(end_ind+1,start_ind+1)),align_to_plot[1,1,:,:].T,color='C1')\n",
    "    p3 = ax.plot(np.flip(np.arange(end_ind+1,start_ind+1)),align_to_plot[0,1,:,:].T,color='C2')\n",
    "    \n",
    "    #ax.set_xticklabels(np.arange(end_ind+1,start_ind+1))\n",
    "    #ax.set_ylabel('$\\\\theta_{{{},{}}}$'.format(CLV,CLV_ind+2))\n",
    "    ax.set_title(titles[j],fontsize=12)\n",
    "    ax.legend([p1[0],p2[0],p3[0]],['$\\\\theta_{1,2}$','$\\\\theta_{2,3}$','$\\\\theta_{1,3}$'],loc='upper right')\n",
    "    plt.grid()\n",
    "    #plt.show()\n",
    "    plt.tight_layout()\n",
    "\n",
    "#plt.savefig('../figures/fig9.eps') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection of CLVs in physical space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot unstable CLVs during persistent states\n",
    "\n",
    "M_ii = 0\n",
    "inds = np.arange(0,n_PCs)\n",
    "\n",
    "fig = plt.figure(figsize=(8,3),constrained_layout=False)\n",
    "\n",
    "State_titles = ['state 1','state 2','state 3']\n",
    "\n",
    "pp = 1\n",
    "for state in np.arange(1,k):\n",
    "    FTLEs_persist_temp = Lyaps_all[M_ii].sel(time =  model.time[state_inds_long[state][0:1]])\n",
    "        \n",
    "    unstable_ind = np.where(FTLEs_persist_temp>0)[0][0]\n",
    "        \n",
    "    CLV_persist_temp = CLVs_all[M_ii].sel(CLV = unstable_ind+1, time =  model.time[state_inds_long[state][0:1]])[inds,:]\n",
    "    \n",
    "    ### CLV has arbitrary direction\n",
    "    ### to keep patterns consistent, manually change the direction where necessary:\n",
    "    ###    CLV_persist_temp = -1*CLV_persist_temp\n",
    "    \n",
    "    CLV_persist_temp = -1*CLV_persist_temp\n",
    "    \n",
    "    CLV_persist_comp = np.mean(np.matmul(CLV_persist_temp.values.T,\n",
    "                               eofs.eofs.loc[0:19,500,:,:].values.transpose(1,0,2)).transpose(0,2,1),axis=2)\n",
    "    \n",
    "    ax = fig.add_subplot(1, 2, pp, projection=ccrs.Orthographic(central_longitude=0.0,central_latitude=90.0))\n",
    "    ax.set_global()\n",
    "    lon, lat = np.meshgrid(lons[100:], lats[0:37])\n",
    "    fill = ax.pcolor(lons[100:-1],lats[0:37],CLV_persist_comp,\n",
    "                     transform=ccrs.PlateCarree(), cmap='PRGn',vmin=-0.05,vmax=0.05)\n",
    "\n",
    "    ax.set_title(State_titles[state] + ' CLV ' + str(unstable_ind+1))\n",
    "    ax.coastlines()\n",
    "\n",
    "    plt.tight_layout()\n",
    "                                                                            \n",
    "    pp += 1\n",
    "    \n",
    "#plt.savefig('../figures/fig5.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures D1-D6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## plot transitions associated with persistent states\n",
    "M_ii = 0\n",
    "\n",
    "theta_labs = ['$\\\\theta_{1,2}$','$\\\\theta_{2,3}$','$\\\\theta_{1,3}$']\n",
    "\n",
    "trans_ind_collect = [trans_1_to_2_inds_CLVs, trans_1_to_3_inds_CLVs,\n",
    "                     trans_2_to_1_inds_CLVs, trans_2_to_3_inds_CLVs,\n",
    "                     trans_3_to_1_inds_CLVs, trans_3_to_2_inds_CLVs]\n",
    "\n",
    "titles  = ['from 1 to 2','from 1 to 3','from 2 to 1',\n",
    "           'from 2 to 3','from 3 to 1','from 3 to 2']\n",
    "\n",
    "for pi in np.arange(0,len(trans_ind_collect)):\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,12),constrained_layout=False)\n",
    "    gs = matplotlib.gridspec.GridSpec(8, 6)\n",
    "\n",
    "    trans_ind = trans_ind_collect[pi][3:4]\n",
    "\n",
    "    Lyap_i = Lyaps_all[M_ii].sel(time = model.time[trans_ind[0]-2:trans_ind[0]+4])\n",
    "        \n",
    "    align_i = align_all[M_ii].sel(time = model.time[trans_ind[0]-2:trans_ind[0]+4])\n",
    "        \n",
    "\n",
    "    ax = fig.add_subplot(gs[0, :])\n",
    "\n",
    "    ax.plot(align_i.time,align_i[0,0,:].T,'.-')\n",
    "    ax.plot(align_i.time,align_i[1,1,:].T,'.-')\n",
    "    ax.plot(align_i.time,align_i[0,1,:].T,'.-')\n",
    "    ax.legend(['$\\\\theta_{1,2}$','$\\\\theta_{2,3}$','$\\\\theta_{1,3}$'],loc='center right',\n",
    "                bbox_to_anchor=(1.02, 0.5))\n",
    "\n",
    "    ax2 = fig.add_subplot(gs[1, :])\n",
    "    ax2.plot(Lyap_i.time,Lyap_i[kk,:]*0,'k')\n",
    "    p1 = ax2.plot(Lyap_i.time,Lyap_i[0,:],'C0.-')\n",
    "    p2 = ax2.plot(Lyap_i.time,Lyap_i[1,:],'C1.-')\n",
    "    ax2.legend([p1[0],p2[0]],['$\\Lambda_1$','$\\Lambda_2$'],loc='center right',\n",
    "            bbox_to_anchor=(1.015, 0.5))\n",
    "\n",
    "    for j in np.arange(0,3):\n",
    "        pp = 0\n",
    "        for dd in np.arange(-2,4):\n",
    "            inds = np.arange(0,n_PCs)\n",
    "        \n",
    "            CLV_trans_temp = CLVs_all[M_ii].sel(CLV = j+1, time = model.time[trans_ind+dd])[inds,:]\n",
    "    \n",
    "            ### CLV has arbitrary direction\n",
    "            ### to keep patterns consistent, manually change the direction where necessary:\n",
    "            ###    CLV_trans_temp = -1*CLV_trans_temp\n",
    "    \n",
    "            CLV_trans_comp = np.mean(np.matmul(CLV_trans_temp.values.T,\n",
    "                                  eofs.eofs.loc[0:19,500,:,:].values.transpose(1,0,2)).transpose(0,2,1),axis=2)\n",
    "    \n",
    "            ax = fig.add_subplot(gs[j*2+2:j*2+4, pp], projection=ccrs.Orthographic(central_longitude=0.0,central_latitude=90.0))\n",
    "            ax.set_global()\n",
    "            lon, lat = np.meshgrid(lons[100:], lats[0:37])\n",
    "            fill = ax.pcolor(lons[100:-1],lats[0:37],CLV_trans_comp,\n",
    "                       transform=ccrs.PlateCarree(), cmap='PRGn',vmin=-0.05,vmax=0.05)\n",
    "\n",
    "            ax.set_title('CLV ' + str(j+1) + ', day ' + str(dd))\n",
    "            ax.coastlines()\n",
    "            \n",
    "\n",
    "            plt.tight_layout()\n",
    "        \n",
    "            pp +=1\n",
    "        \n",
    "    \n",
    "    plt.title(titles[pi])\n",
    "        \n",
    "    #plt.savefig('../figures/figC{}.pdf'.format(pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## choose unstable patterns from above plots\n",
    "inds = np.arange(0,n_PCs)\n",
    "\n",
    "fig = plt.figure(figsize=(10,4),constrained_layout=False)\n",
    " \n",
    "## selected by inspection of above plots\n",
    "trans_ind_ex = [trans_2_to_1_inds_CLVs, trans_2_to_3_inds_CLVs,\n",
    "                trans_3_to_1_inds_CLVs, trans_3_to_2_inds_CLVs]\n",
    "\n",
    "day_ind_ex = [1,1,2,1]\n",
    "\n",
    "CLV_ind_ex = [1,2,2,1]\n",
    "\n",
    "titles = ['A','B','C','D']\n",
    "\n",
    "pp = 1\n",
    "for ti in np.arange(0,len(trans_ind_ex)):\n",
    "    ## check that FTCLE is positive\n",
    "    FTLEs_trans_temp = Lyaps_all[M_ii].sel(FTLE = CLV_ind_ex[ti],\n",
    "                                           time = model.time[trans_ind_ex[ti][3:4]+day_ind_ex[ti]])\n",
    "    \n",
    "    if FTLEs_trans_temp<0:\n",
    "        print('FTCLE {} is negative on '.format(CLV_ind_ex[ti]) + \n",
    "              np.datetime_as_string(model.time[trans_ind_ex[ti][3:4]+day_ind_ex[ti]][0].values, unit='D'))\n",
    "        break\n",
    "        \n",
    "    CLV_trans_temp = CLVs_all[M_ii].sel(CLV = CLV_ind_ex[ti], \n",
    "                                          time = model.time[trans_ind_ex[ti][3:4]+day_ind_ex[ti]])[inds,:]\n",
    "    \n",
    "    ### CLV has arbitrary direction\n",
    "    ### to keep patterns consistent, manually change the direction where necessary:\n",
    "    ###    CLV_trans_temp = -1*CLV_trans_temp\n",
    "    \n",
    "    if ti == 0 or ti == 1 or ti == 2:\n",
    "        CLV_trans_temp = -1*CLV_trans_temp\n",
    "    \n",
    "    CLV_persist_comp = np.mean(np.matmul(CLV_trans_temp.values.T,\n",
    "                               eofs.eofs.loc[0:19,500,:,:].values.transpose(1,0,2)).transpose(0,2,1),axis=2)\n",
    "    \n",
    "    ax = fig.add_subplot(1, 4, pp, projection=ccrs.Orthographic(central_longitude=0.0,central_latitude=90.0))\n",
    "    ax.set_global()\n",
    "    lon, lat = np.meshgrid(lons[100:], lats[0:37])\n",
    "    fill = ax.pcolor(lons[100:-1],lats[0:37],CLV_persist_comp,\n",
    "                     transform=ccrs.PlateCarree(), cmap='PRGn',vmin=-0.05,vmax=0.05)\n",
    "\n",
    "    ax.set_title(titles[ti])\n",
    "    ax.coastlines()\n",
    "\n",
    "    plt.tight_layout()\n",
    "                                                                            \n",
    "    pp += 1\n",
    "\n",
    "#plt.savefig('../figures/fig6.pdf') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## selected by inspection of above plots\n",
    "\n",
    "trans_ind_ex = [trans_2_to_1_inds_CLVs, trans_2_to_3_inds_CLVs, trans_2_to_3_inds_CLVs,\n",
    "                trans_2_to_3_inds_CLVs, trans_3_to_1_inds_CLVs, trans_2_to_3_inds_CLVs,\n",
    "                trans_3_to_2_inds_CLVs]\n",
    "\n",
    "\n",
    "day_ind_ex = [1,1,2,1,2,2,1]\n",
    "\n",
    "CLV_ind_ex = [1,1,2,2,2,1,1]\n",
    "\n",
    "pattern_ex = ['A','A','A','B','C','D','D']\n",
    "\n",
    "transition_ex = ['2 to 1','2 to 3','2 to 3','2 to 3',\n",
    "                 '3 to 1','2 to 3','3 to 2']\n",
    "\n",
    "FTLEs_ex = [[]]*7\n",
    "\n",
    "for ti in np.arange(0,len(trans_ind_ex)):\n",
    "    FTLEs_temp = Lyaps_all[M_ii].sel(FTLE = CLV_ind_ex[ti], time = model.time[trans_ind_ex[ti][3:4]+day_ind_ex[ti]])\n",
    "    \n",
    "    if FTLEs_temp < 0:\n",
    "        print('FTCLE {} is negative on '.format(CLV_ind_ex[ti]) + \n",
    "              np.datetime_as_string(model.time[trans_ind_ex[ti][3:4]+day_ind_ex[ti]][0].values, unit='D'))\n",
    "        break\n",
    "    \n",
    "    FTLEs_ex[ti] = np.round(FTLEs_temp[0].values,3)\n",
    "    \n",
    "df = pd.DataFrame([pattern_ex,transition_ex,day_ind_ex,CLV_ind_ex,FTLEs_ex],\n",
    "                  index = ['pattern','transition','day','CLV','FTCLE'], columns = [' ']*7)\n",
    "\n",
    "\n",
    "df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "(ax1, ax2, ax3, ax4) = fig.subplots(4, 2, gridspec_kw={'width_ratios': [2, 1]})\n",
    "\n",
    "axes_collect = np.array([ax1, ax2, ax3, ax4])\n",
    "\n",
    "pxx_all = [[]]*len(Ms)\n",
    "\n",
    "for j in np.arange(0,len(Ms)):\n",
    "    axes = axes_collect[j]\n",
    "\n",
    "    axes[0].plot(align_all[j].time,align_all[j][0,0,:].T)\n",
    "    axes[0].set_xlim([np.datetime64(\"2010-01-01\"),np.datetime64(\"2017-06-01\")])\n",
    "    axes[0].set_title('M = {}'.format(Ms[j]),fontsize = 13)\n",
    "    \n",
    "    freq, pxx = scipy.signal.welch(align_all[j][0,0,:],nperseg=4084)#,detrend='linear')\n",
    "    peaks_2sd = scipy.signal.find_peaks(pxx,threshold=2*np.std(pxx))[0]\n",
    "    peaks_3sd = scipy.signal.find_peaks(pxx,threshold=3*np.std(pxx))[0]\n",
    "    \n",
    "    axes[1].loglog(freq,pxx/(np.sum(pxx)))\n",
    "    axes[1].loglog(freq[peaks_2sd],pxx[peaks_2sd]/(np.sum(pxx)),'r.',mew=2,ms=6)\n",
    "    axes[1].loglog(freq[peaks_3sd],pxx[peaks_3sd]/(np.sum(pxx)),'rx',mew=3,ms=8)\n",
    "    axes[1].set_xlim([5e-4,5e-1])\n",
    "    axes[1].set_title('M = {}'.format(Ms[j]),fontsize = 13)\n",
    "    \n",
    "    if j == len(Ms)-1:\n",
    "        axes[0].set_xlabel('(a)', fontsize=20)\n",
    "        axes[1].set_xlabel('(b)', fontsize=20)\n",
    "    \n",
    "    pxx_all[j] = pxx\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('../figures/fig10.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment and transition index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate transition index with window equal to push forward (here M=50)\n",
    "j = -1\n",
    "window = Ms[j]\n",
    "y = 0\n",
    "end_ind = model.time.shape[0]-window-5\n",
    "trans_index = np.empty(end_ind)\n",
    "\n",
    "for y_ind in np.arange(0,end_ind):\n",
    "    trans_index[y] = np.count_nonzero(np.isin(model.time[5+y_ind:5+y_ind+window],model.time[trans_ind_all]))/window\n",
    "    \n",
    "    y += 1\n",
    "\n",
    "## convert to xarray\n",
    "trans_index = xr.DataArray(trans_index, coords=[model.time[5+window:5+end_ind+window]], dims=['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = -1\n",
    "fig = plt.figure(figsize=(10,2.5))\n",
    "ax = plt.gca()\n",
    "ax.plot(align_all[j].time,align_all[j][0,0,:].T)\n",
    "ax.plot(model.time[window+5:], trans_index)\n",
    "ax.set_xlim([np.datetime64(\"2009-01-01\"),np.datetime64(\"2018-12-31\")])\n",
    "plt.title('Transition index vs alignment (M={})'.format(window),fontsize=13)\n",
    "plt.legend(['$\\\\theta_{1,2}$','transition index'],loc='lower right')\n",
    "#plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('../figures/fig11.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lagged_correlations(y1, y2, nlags=40):\n",
    "    \"\"\"Calculate lagged correlations of one variable with another.\"\"\"\n",
    "    start_time = max(y1.index.min(), y2.index.min())\n",
    "    end_time = min(y1.index.max(), y2.index.max())\n",
    "    nonlagged_da = y1.where((y1.index >= start_time) & (y1.index <= end_time)).dropna()\n",
    "    lagged_da = y2.where((y2.index >= start_time) & (y2.index <= end_time)).dropna()\n",
    "    mask = np.logical_or(np.isnan(nonlagged_da.to_numpy()), np.isnan(lagged_da.to_numpy()))\n",
    "    nonlagged_values = np.ma.masked_array(nonlagged_da.to_numpy(), mask=mask)\n",
    "    lagged_values = np.ma.masked_array(lagged_da.to_numpy(), mask=mask)\n",
    "    correlations = np.empty((nlags,))\n",
    "    correlations[0] = stats.pearsonr(nonlagged_values, lagged_values)[0]\n",
    "    for i in range(1, nlags):\n",
    "        correlations[i] = stats.pearsonr(nonlagged_values[i:], lagged_values[:-i])[0]\n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = calculate_lagged_correlations(trans_index.to_pandas(),align_all[j][0,0,:].to_pandas(),nlags=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print max correlation and lag\n",
    "max_corr_ind = np.where(abs(corrs) == np.max(abs(corrs)))[0][0]\n",
    "\n",
    "print('Max correlation', round(corrs[max_corr_ind],2),'for',max_corr_ind,'day lag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## separate alignemnt by season\n",
    "seasons = ['DJF','MAM','JJA','SON']\n",
    "align_season = [[]]*4\n",
    "align_season_avg = [[]]*4\n",
    "\n",
    "ii = 0\n",
    "for si in seasons:\n",
    "    align_season[ii] = align_all[-1].where(time_CLVs_all[-1].dt.season==si,drop=True)\n",
    "    align_season_avg[ii] = np.mean(align_season[ii],axis=2)\n",
    "    ii += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12, 12])\n",
    "matplotlib.gridspec.GridSpec(2,1)\n",
    "\n",
    "\n",
    "for axi in np.arange(0,4):\n",
    "    mask =  np.tri(align_season_avg[axi].shape[0], k=-1)\n",
    "    align_season_avg[axi] = np.ma.array(align_season_avg[axi], mask=mask)         \n",
    "  \n",
    "    a1 = plt.subplot2grid((2,2), (int(np.floor(axi/2)),np.mod(axi,2)), colspan=1, rowspan=1)\n",
    "    \n",
    "    n_levels = 10\n",
    "    cmap = matplotlib.cm.get_cmap('seismic',n_levels)\n",
    "    cmap_opaque = np.array(np.zeros((n_levels,4),dtype=np.float))\n",
    "    for c in np.arange(0,n_levels):\n",
    "        cmap_opaque[c,:] = np.array(cmap(c))\n",
    "        cmap_opaque[c,3] = 0.7\n",
    "    cmap = matplotlib.colors.ListedColormap(cmap_opaque)\n",
    "    cmap.set_bad('w',1.)\n",
    "    \n",
    "    cax = a1.matshow(align_season_avg[axi].T,cmap=cmap,vmin=0,vmax=1)\n",
    "    a1.set_xticklabels(np.arange(0,7))\n",
    "    a1.set_yticklabels(np.arange(1,8))\n",
    "    a1.set_title(seasons[axi] + '\\n',fontsize=15)\n",
    "\n",
    "cbar_ax = fig.add_axes([0.93, 0.13, 0.03, 0.75])\n",
    "fig.colorbar(cax, cax=cbar_ax)\n",
    "\n",
    "#plt.savefig('../figures/fig12.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
